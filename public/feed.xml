<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nick Zuber | Software Engineer</title>
    <description>Nick Zuber is a sofware engineer for the web and mobile applications based out of the Boston area.
</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://github.com/pages/nickzuber/ssg-nickzuber.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 29 Jul 2017 22:20:40 -0700</pubDate>
    <lastBuildDate>Sat, 29 Jul 2017 22:20:40 -0700</lastBuildDate>
    <generator>Jekyll v3.4.5</generator>
    
      <item>
        <title>Continuations and Their Role In JavaScript</title>
        <description>&lt;p&gt;By definition, a computer program is a collection of instructions that are
designed to perform a specific task. It is inherently important for a
developer to understand each individual instruction or step the program has to
take in order to reach its goal correctly. Naturally, it can be valuable to a
developer to have more control over these instruction depending on the program
at hand. When the program is executing some specific instruction, the
collection of steps that still need to be processed in order for the program to
complete a computation is known as a continuation. Some languages are able to
provide its users this ability to treat continuations as first class features,
while some other languages require its users to implement them manually.
Regardless of the kind of program being created, continuations can always be a
valuable concept and tool for a developer to understand in order to be equipped
to build smart and sound programs.&lt;/p&gt;

&lt;h2 id=&quot;continuation-passing-style&quot;&gt;Continuation-Passing Style&lt;/h2&gt;

&lt;p&gt;Using continuations to your advantage within computer programs can yield
powerful results. A common method for using continuations is known as
continuation-passing style. In the continuation-passing style paradigm, control
is within the program is passed explicitly by continuations. This is in
direct contrast to more widely used styles of programming such as direct style.
The direct style of programming is can generally be described as synchronous
programming, in which continuations are passed implicitly by just advancing to
the next line of the program. While some languages in particular have native
support for continuations, such that continuations are treated as first class
features, other languages are still able to implement their own interpretation
of continuation-passing style so long as they have the right tools to do so. In
order for a language to properly implement continuation-passing style, the
language needs to be able to support closures and proper tail calls. This
is due to the fact that for continuation-passing style to work properly, the
language needs to be able to manually create continuations which can be
implemented as non-returning functions that are able to encapsulate the
respective name bindings within its lexical scope, and be able to support
functions that are able to call other functions at the very end of themselves.&lt;/p&gt;

&lt;p&gt;One way to implement continuation-passing style into a program is to have a
function which takes in an additional argument of another function, which
represents the continuation. When the first function completes its execution,
instead of returning the result of the procedure, it instead passes this result
into the continuation, thus advancing the program with its product. In
contrast, the direct style of programming would rather return the result of the
function back to the caller in order to further advance the program, while with
the continuation-passing style, the next set of instructions for the program is
being called with the result of the function when its ready.&lt;/p&gt;

&lt;h2 id=&quot;continuations-in-javascript&quot;&gt;Continuations in JavaScript&lt;/h2&gt;

&lt;p&gt;JavaScript is a great example of a language that doesn’t have first class
continuations, but in many cases requires continuation-passing style to be
supported in order to perform many of the useful tasks that this language is
often times expected to be able to do. In JavaScript, manual continuations are
simple to write since the language supports closures, and continuation-passing
style is also easily implemented since the language naturally supports proper
tail calls. By simply passing in a function argument, which represents the
continuation of the program, to another function, which represents the current
process being executed, we’re able to pass the result of the current function
into the continuation and invoke it at the very end of the current function,
hence advancing the program in a continuation-passing style.&lt;/p&gt;

&lt;p&gt;For JavaScript related software development, continuation-passing style is a
very important topic, primarily due to the fact that JavaScript itself is
single threaded. This means that all of the programs being run on an instance
of a JavaScript runtime engine is done so on a single thread, which implies
that any halt or waiting that must occur within a given JavaScript process will
halt the entire program. This problem is clearly noticeable for any IO related&lt;br /&gt;
tasks that involve some JavaScript program waiting on some input before it can
complete a task. Continuation-passing style solves this single threaded problem
by deferring the continuation of the program to be executed when its ready,
rather than having to freeze the entire program waiting for it to be ready.
This approach is widely used, and is quite necessary, for asynchronous
programming in not just JavaScript applications, but any program that might use
this method of programming. Asynchronous procedures are programs that run
independently of the main thread or process in a non-blocking manner. This
process might be reminiscent of multithreading, however asynchronous programs
can be implemented by alternative means, therefore making them very different;
for example, JavaScript supports asynchronous methods and JavaScript runtime
engines are single threaded.&lt;/p&gt;

&lt;p&gt;The concept of asynchronous programming inherently lends itself to
continuation-passing style; if a program is running independently of the main
thread, how will the main program know when it has finished? The most common
answer to this problem is to implement continuation-passing style for
asynchronous functions. The idea is to pass in the continuation of the program
to the asynchronous function, and once the function has completed its process,
the result is then passed as an argument into the continuation, and the program
can advance. All of this can occur without the program having to halt, since we
aren’t waiting for the call to finish to continue running the program, but
rather just giving the asynchronous call a set of instructions to perform with
its result once it has completed.&lt;/p&gt;

&lt;h2 id=&quot;in-closing&quot;&gt;In Closing&lt;/h2&gt;

&lt;p&gt;Being able to have control over the state of a given program can be a very
useful advantage for a developer to have. Even in cases where continuations
aren’t required in order to build a program, they still provide an alternative
way of thinking and going about certain problems. Continuations can also help
solve programming efficiency problems, like in the case of asynchronous
JavaScript programming, where we would otherwise have impractical alternatives.
Therefore, regardless of the kind of program being created, continuations can
always be a valuable concept and tool for a developer to understand in order to
be equipped to build smart and sound programs.&lt;/p&gt;
</description>
        <pubDate>Thu, 22 Dec 2016 10:34:08 -0800</pubDate>
        <link>/blog/continuations-and-their-role-in-javascript</link>
        <guid isPermaLink="true">http://github.com/pages/nickzuber/ssg-nickzuber.com/blog/continuations-and-their-role-in-javascript</guid>
        
        
      </item>
    
      <item>
        <title>Taking a Shallow Dive into JavaScript</title>
        <description>&lt;p&gt;When it comes to choosing a primary programming language for a project or
professional application, there are plenty of different factors that need to be
considered. Among those, there are also different types or categories of
programming languages to choose from as well; some of which being scripting or
interpreted languages and compiled languages. While each flavor of programming
language has their own weaknesses and strengths, one particular scripting
language, known as JavaScript, is rapidly growing and becoming more ubiquitous
across all kinds of programming stacks. Starting out as a simple language used
for the web, JavaScript has quickly become much more than that; now appearing
in all sorts of different kinds of applications like machine learning programs
and can even be used for writing servers. JavaScript is easily one of the most
versatile and powerful scripting languages in modern programming, and can be
used to build strong and scalable applications.&lt;/p&gt;

&lt;h2 id=&quot;programming-paradigm--continuations&quot;&gt;Programming Paradigm &amp;amp; Continuations&lt;/h2&gt;

&lt;p&gt;One of the many characteristics of JavaScript that make this language so unique
is its flexible support for a multitude of programming paradigms. At its core,
just about everything in JavaScript is an object that is implemented as an
associative array, with the exception of its six primitive types: string,
number, boolean, null, undefined, and symbol. Even then, most of these
primitive types have their respective object wrapper classes that allows
developers to declare these primitives as objects. Now, since almost everything
in JavaScript is itself an object, it’s clear to see how object oriented
programming paradigms are natively supported by the language. The one caveat
here, however, is that JavaScript doesn’t support classical inheritance, but
rather prototypical inheritance. This means that each object has its own
prototype which can be composed to properties and methods, and since objects
are mutable, to achieve inheritance you just create a new instance of an object
and augment its properties.&lt;/p&gt;

&lt;p&gt;JavaScript can also be treated as an impure functional language since it
natively supports first class lambda functions, lexical closures, immutability,
and tail recursion. While JavaScript itself doesn’t overtly promote functional
programming or immutability, it’s very possible and becoming quite common to
use JavaScript in a functional manner. Functions in JavaScript are treated as
objects themselves, and can be used as arguments to other functions as well as
having additional properties and methods. Aside from this feature, each
function is bound with its own lexical scope that includes any of the variables
declared within the outer function that it was declared inside of. With
this ability to have higher ordered functions and tail recursion, JavaScript is
able to support continuation-passing style.&lt;/p&gt;

&lt;p&gt;In JavaScript related software development, continuation-passing style is a
very important topic, primarily due to the fact that JavaScript itself is
single threaded. This means that all of the programs being run on an instance
of a JavaScript runtime engine are done on a single thread, which implies that
any halt or waiting that must occur within a given JavaScript process will halt
the entire program. This problem is clearly noticeable for any IO related tasks
that involve some JavaScript program waiting on some input before it can
complete a task. Continuation-passing style solves this single threaded problem
by deferring the continuation of the program to be executed when its ready,
rather than having to freeze the entire program waiting for it to be ready.
This approach is widely used, and is quite necessary, for asynchronous
programming in not just JavaScript applications, but any program that might use
this method of programming. Asynchronous procedures are programs that run
independently of the main thread or process in a non-blocking manner. This
process might be reminiscent of multithreading, however asynchronous programs
can be implemented by alternative means, therefore making them very different;
for example, JavaScript supports asynchronous methods and JavaScript runtime
engines are single threaded.&lt;/p&gt;

&lt;p&gt;The concept of asynchronous programming inherently lends itself to
continuation-passing style; if a program is running independently of the main
thread, how will the main program know when it has finished? The most common
answer to this problem is to implement continuation-passing style for
asynchronous functions. The idea is to pass in the continuation of the program
to the asynchronous function, and once the function has completed its process,
the result is then passed as an argument into the continuation, and the program
can advance. All of this can occur without the program having to halt, since we
aren’t waiting for the call to finish to continue running the program, but
rather just giving the asynchronous call a set of instructions to perform with
its result once it has completed.&lt;/p&gt;

&lt;h2 id=&quot;type-system&quot;&gt;Type System&lt;/h2&gt;

&lt;p&gt;By default, JavaScript is an untyped language; meaning that it inherently has
no static type system within the native implementation of the language itself.
While the term &lt;em&gt;untyped&lt;/em&gt; has a few different meanings, in the context of
JavaScript it implies that all expressions, functions, and variables belong to
a single type such that a program is always generated where all of the types
match up. Since there is no native static type system, this language can
also be considered as a dynamic language, such that the types of its variables
are volatile and are allowed to change. This inherently leads to programming
bugs that are difficult to trace and can also lead to unexpected behavior if a
variable changes type somewhere within the program and isn’t accounted for.&lt;/p&gt;

&lt;p&gt;This is the primary reason that makes writing fast JavaScript programs hard;
since types are dynamic, the interpreter executing a JavaScript program must
include additional checks during runtime to ensure that variable’s data type.
With statically typed languages, the compiler knows the data types of each
variable in the program and is able to perform these checks very quickly,
usually as a simple lookup in a table or as a static function. In dynamically
typed languages, the interpreter has to update and perform additional checks
each time a variable changes due to its volatile nature, which slows down the
interpreting process. To be more specific, each variable needs to have its own
type checks since it can change data types on the fly, each statement needs to
check to see if it throws an exception due to the possibility of type
mismatching, and since every field can be added, removed, or altered during
runtime, there needs to be checks for that as well. One advantage to
JavaScript being a prototypical object oriented language is that we don’t need
to worry about class hierarchy checks during runtime since each object has its
own copy of its proto chain during its instantiation.&lt;/p&gt;

&lt;h2 id=&quot;garbage-collection&quot;&gt;Garbage Collection&lt;/h2&gt;

&lt;p&gt;Unlike languages such as C that have a very low level and manual way of
handling memory allocation and deallocation, JavaScript uses garbage collection
to reclaim unused memory within a program. In all modern iterations of
JavaScript, this method of garbage collection is often implemented using a mark
and sweep memory allocation strategy. It’s important to note that some older
implementations of JavaScript, notably versions that were created for Netscape,
utilized a reference counting strategy for garbage collection, however this
approach is no longer used today.&lt;/p&gt;

&lt;p&gt;The mark and sweep garbage collector is able to navigate through all of the
variables defined within the program periodically, and mark all of the values
referred to by these variables. This particular algorithm makes sure it
recursively check and mark the values and properties of any objects or arrays
it finds as well in the hopes of being able to find every variable and value
within the program that is still reachable. Therefore, any values or variables
that were not marked is considered garbage that can be reclaimed by the
collector. The garbage collector then begins to sweep through the program,
making sure to deallocate and reclaim any memory it finds that is unmarked and
considered to be garbage. It does this by iterating through each value within
the program’s environment, deallocating any that it finds to be unmarked.&lt;/p&gt;

&lt;p&gt;Traditional and naive implementations of mark and sweep garbage collection tend
to go through complete marking and sweeping phases at a time; the downside to
this approach is that it can hinder the program’s performance since it needs to
start and stop while the garbage collection is happening. In modern iterations
of JavaScript, this method of garbage collection is handled more efficiently by
trying to only run in the background of a program during the runtime. This
helps avoid disrupting the performance of the application while at the same
time still being able to collect any garbage generated during the lifetime of
the program.&lt;/p&gt;

&lt;h2 id=&quot;in-closing&quot;&gt;In Closing&lt;/h2&gt;

&lt;p&gt;Over the last couple of decades, JavaScript has come a long way from being a
simple scripting language built in roughly ten days 9]. It is quickly becoming
a more mature and ubiquitous across all different kinds of programming stacks,
and is capable of much more than it was originally intended when it was first
conceived. JavaScript is easily one of the most versatile and powerful
scripting languages in modern programming, and can be used to build strong and
scalable applications.&lt;/p&gt;
</description>
        <pubDate>Thu, 15 Dec 2016 10:22:51 -0800</pubDate>
        <link>/blog/taking-a-shallow-dive-into-javascript</link>
        <guid isPermaLink="true">http://github.com/pages/nickzuber/ssg-nickzuber.com/blog/taking-a-shallow-dive-into-javascript</guid>
        
        
      </item>
    
      <item>
        <title>The Significance of Type Systems in Programming Languages</title>
        <description>&lt;p&gt;No matter what programming language is being used or how careful the developer
is while using it, human error is always an unavoidable issue when building out
programs of any scale. Among some of the popular approaches to mitigate the
problem of human error, integrating type systems into a programming language
can be a powerful way to go. Type systems introduce the concept of assigning
types to different constructs within a program such as variables, functions,
and expressions, and enforce consistency among the rules associated with each
type. A type can be understood as just a classification of data which has an
interface associated with it that describes how it is intended to be used. With
a type system, the program is able to avoid type related errors, such as
performing an operation on a type that does not support that operation. While
there are many tradeoffs and design decisions that go into implementing a type
system in a programming language, considering a type system and weighing the
different tradeoffs relative to the program being built is an important
decision to consider, and the benefits can be very worthwhile.&lt;/p&gt;

&lt;h2 id=&quot;benefits-of-type-systems&quot;&gt;Benefits of Type Systems&lt;/h2&gt;

&lt;p&gt;There are plenty of benefits to reap from having a type system in a programming
language, both qualitatively and quantitatively. In general, having the ability
to explicitly define types of variables and functions can greatly increase
readability and make the author’s intent more clear; at the end of the day,
humans are the ones reading and writing the code, so this benefit cannot be
easily overlooked. By reducing any ambiguity, a byproduct of manifest typing
can also be the improve of the development process by making it more clear as
to what the program is doing and how it’s doing it.&lt;/p&gt;

&lt;p&gt;One of the primary advantages to having a type system is the elimination of
type errors before any program runs. A type system will analyze the code during
the compilation process and flag any type related errors that it finds.
This makes it so any code that makes it through the type checking process and
gets run is type safe. The entire process of type checking a program before it
runs is to mitigate human errors that are written into the code during the
development process. Without a type system to find and catch these errors, the
only way they would be found in the future is if they were encountered during
the program’s life cycle, which isn’t practical for production code that is
indented to be released to users who expect bug free programs. This is clearly
not ideal, and this benefit alone can be a strong argument for utilizing a type
system.&lt;/p&gt;

&lt;p&gt;Alongside the benefit of finding bugs before code is run, ensuring type safe
code can also have a small boost on performance in certain situations. This is
because if the developer knows what the type of a variable or expression is,
there’s no need for manual type checking within the program itself; this manual
type checking pattern slows down the performance of the program to some degree,
therefore when eliminating the need for this entirely, the program can run
faster. With this in mind, if a type system is implemented efficiently enough,
then it can absolutely be worthwhile to use.&lt;/p&gt;

&lt;h2 id=&quot;drawbacks-and-tradeoffs&quot;&gt;Drawbacks and Tradeoffs&lt;/h2&gt;

&lt;p&gt;While there are many advantages to having guarenteed type safety in programs,
there are drawbacks and other tradeoffs that need to be considered. One of the
more prominent downsides to a type system is that it can reject some correct
programs. It’s not uncommon to encounter a program that works as intended and
is objectively correct in the sense of how it should run and perform, but still
have a type system reject and find errors within it. This has to do with
how type systems are traditionally implemented; there are many different types
of shortcuts a type system has to implement in order to perform efficiently
enough to be worthwhile to use. For example, when analyzing &lt;em&gt;if&lt;/em&gt; statement, if
the type checker was to consider both branches separately and check them
individually, it’s clear to see how the work that the type checker has to do
would double every time it encounters this situation, effectively creating
potentially exponential growth. One way to avoid this problem is to require
both branches to be of the same type; this solves the issue but as a result,
makes it so correct programs that happen to break this rule will be rejected by
the type system.&lt;/p&gt;

&lt;p&gt;With that in mind, its clear to see one of the major issues with type systems.
It’s hard to make a system that accepts as many correct programs as possible
while still being fast enough for it to be worthwhile to use. If a type system
is too slow, then most of the benefits it provides can become moot; when the
type checking process takes longer than just running the program, it can become
less efficient to bother running the type checker than just running the
program.&lt;/p&gt;

&lt;h2 id=&quot;type-inference-and-polymorphism&quot;&gt;Type Inference and Polymorphism&lt;/h2&gt;

&lt;p&gt;Without manifest types being explicitly defined on a variable or expression, it
is still possible to assign those constructs types. The concept of trying to
assign every variable or expression some type such that the type checking
process is successful, and reject the program if no solution can be found, is
known as type interference. While there are many different variations of
implementations for type inference, a good example for how it works would be
how its traditionally implemented in ML. Firstly, types of bindings are
generally determined in the order in which they appear, which is why it’s
important to write any functions or methods before the functions that use them;
without doing so, the inferred types will not be in the environment by the time
they are needed and the system will simply throw a type error. When a
binding is being inferred, all of the relevant definitions and facts are
analyzed and used to determine what the type can possibly be; for example, if
some variable &lt;em&gt;x&lt;/em&gt; is used with the addition operator on some number, it can be
inferred that &lt;em&gt;x&lt;/em&gt; must be of the type integer. If any of these facts that were
gathered happen to conflict with one another, this implies that there exists a
type error in the program.&lt;/p&gt;

&lt;p&gt;Even after this inference analysis process, it is still possible for an
expression or variable to still not have enough constraints associated to it
for the type checker to determine its type. In this situation, it’s common to
assign this expression or variable a polymorphic type by giving it a fresh and
arbitrary type. This new type generally implies that it can hold any type
value; this is commonly seen when a function parameter is never used within the
function, therefore it’s clear that this can be anything and has no constraints
associated with it. Type inference is a useful tool when implemented well; it
adds the benefit of having less verbose code while maintaining the type safety
of the program. However, type inference isn’t perfect; problems don’t arise
often, but when they do, they tend to be particularly hard to debug. When an
expression or variable is inferred with the wrong type, errors can occur well
past the actual spot that is causing the error, which can lead to strange and
unhelpful bugs.&lt;/p&gt;

&lt;h2 id=&quot;in-closing&quot;&gt;In Closing&lt;/h2&gt;

&lt;p&gt;Type systems can be a very powerful and useful asset to any programming
language under the right circumstances. It’s always important to weigh the
different kinds of tradeoffs when considering whether or not a type system
would be worthwhile to include in a given project or program, as there are
clearly advantages to having a type system as well as not having one. With the
use of other strategies, such as type inference, having a type system can be
beneficial without any added work of manually adding type annotations, so there
is little extra work that needs to be done to achieve type safety. With all of
this in mind, it’s always important to at least consider the possibility of
incorporating a type system in any project.&lt;/p&gt;
</description>
        <pubDate>Fri, 09 Dec 2016 10:21:44 -0800</pubDate>
        <link>/blog/the-significance-of-type-systems-in-programming-languages</link>
        <guid isPermaLink="true">http://github.com/pages/nickzuber/ssg-nickzuber.com/blog/the-significance-of-type-systems-in-programming-languages</guid>
        
        
      </item>
    
      <item>
        <title>Automated Memory Management Systems with Garbage Collection</title>
        <description>&lt;p&gt;When creating any type of program or application, the developer almost always
needs to consider memory usage. Whether it is a simple and small program, or a
large and complex program, developers are going to have to worry about managing
their memory consumption in one way or another. Problems can arise when
mistakes are made; remembering to allocate and deallocate can become
nontrivial, especially as the complexity of a program expands. With this
problem in mind, garbage collectors were created; these tools act as a way to
automate the process of reclaiming memory in a program by realizing when
certain memory is no longer being used by the program and frees it to be used
elsewhere. While there are many different techniques and strategies for
solving this problem of automatically collecting memory which is no longer
being used, garbage collection in general benefits the developer in many
situations so long as they are willing to give up, in many cases, some
performance and resources.&lt;/p&gt;

&lt;h2 id=&quot;reference-counting&quot;&gt;Reference Counting&lt;/h2&gt;

&lt;p&gt;One well known technique for tracking memory and trying to figure out when to
deallocate it, is called reference counting. This refers to the process of
literally counting and keeping track of the number of references, or pointers,
there are to a particular block of data or object. The idea behind this is that
when the reference count to an object reaches zero at any point, we can assume
that there are no more references of this object anywhere else in the program,
therefore there is no possible way for the developer to access it. Since it is
now impossible for the developer to call for this data, we can assume this data
is now inaccessible and therefore can be freed. It is important to note that if
an object has been deallocated for any reason, all other objects in the program
which were being referenced by that now removed data, if any, must now all have
their reference counters decremented. This implication leads to a slippery
slope where removing a single object can potentially lead to an arbitrarily
large number of other objects being removed as well, or at least having all of
their reference counters decrement.&lt;/p&gt;

&lt;p&gt;Reference counting is helpful when we want to find unused memory as soon as it
becomes available. This could be important in programs that have a relatively
tight restriction on memory, which therefore would need to deallocate memory
whenever it becomes available in order for program to run correctly. Besides
this feature, reference counting also shifts the responsibility of freeing
memory from the developer and does it automatically. This is generally a good
feature since it can sometimes be unreliable to have to manually free memory in
a program since often times a developer can easily make a mistake and either
forget to delete unused memory or create other types of memory leaks.&lt;/p&gt;

&lt;p&gt;The costs to reference counting can hurt performance and increase memory usage
when programs become rather large. Since we need to keep track of each
reference, we need to keep in mind that for each object in a given program, we
need to use additional memory to store its respective reference count. When
talking about programs with thousands of objects, this overhead can impact the
available memory drastically and may lead to problems down the road. It is also
important to realize that references to objects are generally adjusted quite
frequently, especially when considering one of the downsides previously
mentioned (where removing one object may result in having to decrementing many
other objects’ reference counters). This frequent updating can hinder the
performance of a program and ultimately lead to other speed related issues.
Despite having effectively removed the responsibility for developers to free
memory, we replace that task with having the developers manually track the
number of references by updating each time they add or remove a reference. This
almost contradicts the benefit of not having to manually free memory, since
developers can still make mistakes here by incorrectly adjusting reference
counts the wrong way, or forgetting to do so entirely. It is also important to
note that reference counting is not perfect either; reference counting fails
when it encounters cyclic objects, such that two objects reference each other.
For example, if we simply consider a circular singly linked list such that
the tail of the list points to the second element, every element of this list
will have a reference count of one, with the exception of the second element
having a reference count of two (the head and the tail both pointing to it).
Now, if we happen to destroy the head of the linked list, every other element
of the list will all have a reference count of one, despite there being no
possible way to access this list. Hence, we clearly have a list of objects that
are considered garbage and should have their memory freed, yet they still have
reference counts of one.&lt;/p&gt;

&lt;h2 id=&quot;mark--sweep&quot;&gt;Mark &amp;amp; Sweep&lt;/h2&gt;

&lt;p&gt;With this problem of collecting cyclic data in mind, other garbage collection
techniques have been developed aimed to solve this. One popular technique to
garbage collection, and the first algorithm that was able to solve the cyclic
data problem, is known as mark and sweep. In this approach, the garbage
collection process itself is divided into two different parts: the marking
phase and the sweeping phase. Before any marking or sweeping is done, it is
important to note that for every object that is created, it is decorated with a
single bit that will only be used for this garbage collection process. This bit
is initially set to zero before the actual mark and sweep process begins.&lt;/p&gt;

&lt;p&gt;Firstly we have the marking phase: in this phase of the process, we create a
collection of &lt;em&gt;roots&lt;/em&gt;, which consist of all the initial pointers that are being
held in registers. When we have this collection, we perform a depth first
search on the roots, marking every object that we are able to find. These
marked objects represent all of the reachable objects within the given program.
The next phase is the sweep phase: this is where each object that is being
stored in memory is check to see if it had been marked or not. Every object
that isn’t marked is put into a queue that keeps track of locations in memory
we can free later down the road if we ever need more memory. Mark and sweep is
only invoked when the program runs out of memory, therefore the process does
not hinder the performance of the program unnecessarily. This also implies that
the unmarked blocks of memory which need to be freed aren’t &lt;em&gt;actually&lt;/em&gt; freed
until the program needs more memory.&lt;/p&gt;

&lt;p&gt;There are a few problems with this approach that should immediately stick out;
one of which being how the garbage collection process doesn’t start until the
program needs more memory, but we would need some memory to store the list of
reference roots. Since this list is arbitrarily large, there is no way to
reserve a spot in memory for this list in advance. Similarly to this problem,
the list of unmarked memory which can be freed would also need to be stored
somewhere. However, there are some clever ways these issues can be resolved:
instead of storing the reference roots in a list, this data can be stored
within the objects’ memory themselves with a technique known as pointer
reversal. This approach can be used in the place of storing a list of root
references; the idea here is to have each reference we visit (during the depth
first traversal of the marking phase) point back to its parent, so that when we
get to the end of a particular reference chain, we can find our way back to the
initial root reference and continue the process.&lt;/p&gt;

&lt;p&gt;Solving the issue with tracking the memory blocks that can be freed is somewhat
similar in nature, but relatively simpler; since this data is no longer
important, we can write over some of it without consequence, so this allows us
to mark part of memory as perhaps the size of the block followed by the
location in memory of the next memory block to free. This way, we don’t need to
use any auxiliary memory for tracking the list of memory that can be freed.&lt;/p&gt;

&lt;h2 id=&quot;stop--copy&quot;&gt;Stop &amp;amp; Copy&lt;/h2&gt;

&lt;p&gt;Another popular method of garbage collection is what’s known as stop and copy.
With this approach, the memory for a given program is divided into two
sections: an old space and a new space. The idea behind this approach is to
use the memory in the old space until it’s full, copy over the reachable object
references into the new space, then free all the remaining memory in the old
space (since we can assume the memory left over is garbage since its
unreachable), and then switch the roles of the two spaces and repeat. One of
the great features of this approach is its fast and simple allocation strategy;
there’s a single heap pointer in the old space which points to the next
available location in memory. As that block of memory gets used, this heap
pointer simple increments to the next location in the space.&lt;/p&gt;

&lt;p&gt;However, one of the downsides to this approach is the difficult challenge that
comes with deep reference copying. When some object gets copied into the new
space, all of the objects that it points to are also marked as reachable and
therefore copied into the new space as well. The challenging part here is to
fix the pointers to make sure that the object copies point to the correct
object copies. There are ways to solve this problem, one of which being a
technique using forwarding pointers and partitioning the new space further into
additional sections. Here, the goal is to copy an object into the new space,
marking the old reference as a copied element and adding a forwarding pointer
from the old reference to the newly copied reference. We then process all of
the pointers in the new block we had just copied, and as we copy its references
we fix its pointers to point to the new copies instead of the old reference.
Despite this algorithm being relatively complicated, the reason why this
approach isn’t ideal isn’t because of the complexity, but rather because of the
expensive nature of all these operations. It’s also important to note that a
large portion of the memory in a program which implements a stop and copy
garbage collection is dedicated to the garbage collection process itself, which
inherently limits the finite amount of memory within the program.&lt;/p&gt;

&lt;h2 id=&quot;in-closing&quot;&gt;In Closing&lt;/h2&gt;

&lt;p&gt;The ability to automate the process of deallocating and freeing unused memory
in a program can be beneficial to developers and help mitigate the human error
that comes into play when having to deal with memory management manually. Like
most great things, there are always important tradeoffs to consider; in the
case of garbage collection, we know that these processes can take additional
resources to work properly, slow down programs and hinder performance, and
ultimately are not perfect solutions.&lt;/p&gt;
</description>
        <pubDate>Wed, 02 Nov 2016 16:34:57 -0700</pubDate>
        <link>/blog/automated-memory-management-systems-with-garbage-collection</link>
        <guid isPermaLink="true">http://github.com/pages/nickzuber/ssg-nickzuber.com/blog/automated-memory-management-systems-with-garbage-collection</guid>
        
        
      </item>
    
      <item>
        <title>What's OCaml and Why You Should Care</title>
        <description>&lt;p&gt;In a world where there are hundreds of different programming languages to
choose from, choosing the correct languages for a project or task can be
overwhelming and difficult. With this being said, there are certainly different
flavors of programming languages geared towards solving different types of
problems. Among these, there is a family of general purpose functional
languages called MetaLanguage (or ML as it’s commonly referred to). Within the
family of MetaLanguage, there exists a uniquely interesting programming
language called Objective Categorical Abstract Machine Language, which is
commonly referred to as OCaml. OCaml was invented in 1996 by the Inria
corporation. This language is primarily an extension of the Caml language
with the inclusion of some object-oriented design. OCaml’s tool set and
features are what make it a robust and powerful language to use in modern day
applications.&lt;/p&gt;

&lt;h2 id=&quot;programming-paradigm&quot;&gt;Programming Paradigm&lt;/h2&gt;

&lt;p&gt;OCaml is a general purpose functional programming language. By virtue of this,
the language provides first class functions; this implies that functions are
treated as proper variables. First class functions open the door to more
powerful methods of functional programming; OCaml is able to have higher order
functions, anonymous functions, and currying. Despite the functional paradigm
that’s encouraged by the language itself, what makes this particular language
useful is its support for both imperative styles of programming as well as
object-oriented programming.&lt;/p&gt;

&lt;p&gt;Imperative programming styles are, for the most part, fairly accounted for and
supported in OCaml. This particular feature is useful because it ultimately
gives the developer greater control over the memory representation of data and
how the program itself is executed. The language provides access to variables
and arrays; it can also record components which can be declared as mutable.
This impure functional language might allow irresponsible developers to shoot
themselves in the foot, but it also add more power to the language. The
imperative support allows OCaml program to have an extra layer of data control,
which consequently opens the door to more powerful abilities such as iteration
and rich data structures such as hashmaps.&lt;/p&gt;

&lt;p&gt;The object-oriented features of OCaml allow developers to create objects and
classes for their programs. Similarly to how the support for imperative
programming styles benefits OCaml, the object-oriented features allow for data
organization in classes and objects, and even goes as far as to provide more
robust properties like polymorphic classes. Since OCaml’s object-oriented
pattern allows for polymorphism, this gives developers the ability to create
more complex programs and create abstract types.&lt;/p&gt;

&lt;p&gt;Aside from the supported paradigms and styles of programming, OCaml, much like
all C and ML languages, is strict by default in the sense that it enforces
eager evaluation. Many benefits can be derived from strict languages such as
OCaml; one major point being that there is no overhead for having to keep track
of evaluations much like there is in a non-strict or lazy language. OCaml has a
strict evaluation pattern which implies that it has more control over when
expressions are evaluated; this can help with catching bugs. Examples of this
can be attempting to divide by zero in a function argument; with strict
evaluation, this argument gets evaluated first and the error is caught, whereas
with a lazy evaluation, this error isn’t caught until the function itself is
executed.&lt;/p&gt;

&lt;h2 id=&quot;type-system&quot;&gt;Type System&lt;/h2&gt;

&lt;p&gt;Some of the more uniquely interesting and incredible aspects of OCaml include
its statically typed system, and its ability to leverage this to help produce
extremely performant code. The good thing about statically typed languages is
that they help eliminate type conflicts at compile time rather than at runtime.
This guarantee of safety ensures that programs are verified before they’re ever
executed. This feature is particularly helpful because it prevents very
common human errors that occur when developing a program: conflating an integer
and pointer, accessing an invalid piece of data, or buffer overflows, just to
name a few.&lt;/p&gt;

&lt;p&gt;Performance is a rather large benefit we get from OCaml’s statically typed
system. Since any type conflicts are resolved at compile time, this eliminates
any need for type related safety checks at runtime. These kinds of checks are
necessary for dynamically typed languages, which consequently add a slow
overhead, therefore slowing down the speed of the program itself. Most
scripting languages tend to be dynamically typed, so this simple change of
type system puts OCaml far ahead of its peers with respect to speed. When it
comes to benchmarking, OCaml is argued to be almost as fast as C, which is
incredibly impressive and unique considering it is a type safe language.&lt;/p&gt;

&lt;p&gt;Not only is OCaml statically typed, but it also has type inference, which is
the ability to automatically assume the data type of an expression at compile
time, without its type needing to be explicitly defined. Rather, the compiler
will generally either attempt to predict the type based on existing atomic data
types of the language, or it will try to gather information about the data
being analyzed, and come to a conclusion that way. As a matter of fact, entire
programs can be written without explicitly declaring &lt;em&gt;any&lt;/em&gt; types, and OCaml is
able to figure out all of the types automatically. Traditionally, this power
comes at a cost; type inference is usually a relatively expensive feature to
support, and can be expected to ultimately slow down the compilation process.
In OCaml, however, this isn’t the case due to its unique and revolutionary
implementation of its type inference algorithm. OCaml has an interesting take
on the usual approach; its own algorithm is a derivation of the common type
inference algorithm known as Algorithm W, only OCaml’s interpretation utilizes
a graph based algorithm which ends up being a much more speed efficient
variation of the traditional approach. The downside to the quick inference
implementation, is that it can result in strange type errors in more
complicated programs.&lt;/p&gt;

&lt;h2 id=&quot;modules-and-functors&quot;&gt;Modules and Functors&lt;/h2&gt;

&lt;p&gt;Alongside these features, OCaml also comes with a mature module system that can
be categorized into three key parts: signatures, structures, and functors. The
signature of a module defines what type of data it can be parameterized with,
where the structure relates to how the body of the module is designed. Functors
are a very powerful feature in this module system; this is gives developers the
ability to parameterize a module with &lt;em&gt;other&lt;/em&gt; modules. This opens the door
to all sorts of complex programs; having modules that can be constructed using
one or more other modules adds layers of abstraction that otherwise wouldn’t be
achievable in OCaml. A simple use case of functors could be the following:
let’s say you have a module that creates a set of items, and in order to check
if an item already exists within a given set, we need to apply a custom
equality function. We can create an equality module and use it to parameterize
our set module so that we can apply our custom equality function within the set
module; this makes the set module a functor. Besides the support of functors,
this module system can also be particularly helpful to developers for the
simple added bonus of having namespacing within modules; this can help avoid
any naming conflicts that may occur in a program.&lt;/p&gt;

&lt;h2 id=&quot;structure-and-tool-set&quot;&gt;Structure and Tool Set&lt;/h2&gt;

&lt;p&gt;OCaml provides an extensive set of tools and libraries when it comes to
development, one of which being the interactive toplevel interpreter; this
system is designed in a read-eval-print loop where it repeatedly reads
expressions from the input, type checks, compiles, evaluates, then prints the
result. This is similar to how Racket’s interactive toplevel interpreter
works (or Node.js, or most other interactive toplevel interpreters for that
matter). Having this kind of tool makes the developer’s life very easy when
they want to just test out some code quickly. The toplevel interpreter is also
used to interpret entire files of OCaml code; the general syntax is &lt;code class=&quot;highlighter-rouge&quot;&gt;ocaml
options objects scriptfile&lt;/code&gt;. Before a scriptfile is read by the interpreter,
OCaml will first search the current directory, and, if not found, then the
&lt;code class=&quot;highlighter-rouge&quot;&gt;HOME&lt;/code&gt; directory for a &lt;code class=&quot;highlighter-rouge&quot;&gt;.ocamlinit&lt;/code&gt; file. This file can be used to import
necessary libraries that the program might need, or provide any other set up
related tasks.&lt;/p&gt;

&lt;p&gt;OCaml also provides the developers with a bytecode compiler and a native code
compiler. Similar to a C compiler, dependency ordering matters when trying to
link and compile several files. This compiler will build and provide optimized
executable files that generally run faster than simply interpreting the source
code, with the native code compiler usually being faster. The main purpose
of these compilers is for both portability and efficiency; OCaml is able to
build your files into very fast executables which, as we’ve mentioned earlier,
are roughly the same speed as C. There are some third party tools that
assist the developer in linking dependencies, one of the more popular ones
being &lt;code class=&quot;highlighter-rouge&quot;&gt;ocamake&lt;/code&gt;; you feed it your source files and it will link them together
automatically and the build them into an executable using the bytecode
compiler. Tools like these alongside the compilers themselves really benefit
the language in both efficiency and with being able to move code around on
different systems.&lt;/p&gt;

&lt;p&gt;Managing packages in OCaml is relatively easy due to the open source package
manager called OPAM. This package manager supports multiple simultaneous
compiler installations and also integrates well with a git based development
workflows. OPAM is a way for engineers to easily install and share any open
sourced packages that they create and publish to the OPAM package registry,
which helps increase the speed of the development process by sharing commonly
used code. Especially for an emerging language like OCaml, the ability to
simply distribute code and build a strong set of libraries really help the
language grow and thrive as more developers begin to use it.&lt;/p&gt;

&lt;h2 id=&quot;in-closing&quot;&gt;In Closing&lt;/h2&gt;

&lt;p&gt;OCaml is a powerful, general purpose functional programming language that has a
wide support for different needs. With its support of both imperative and
object-oriented programming paradigms, it’s needless to say that this language
is very flexible when it comes to what the developer might need. With its
statically typed system and very efficient implementation of its type inference
algorithm, leveraging the native code compiler can result in OCaml programs
turning out to be almost as fast as C programs. With these strong
characteristics and features, it’s no wonder that OCaml is quickly growing in
the open source software community; Facebook’s JavaScript static type checker,
Flow, is written in OCaml and proves to be one of the most revolutionary
tools a JavaScript developer can use. From using OCaml’s robust tool set and
speedy execution, this software has quickly become one of the most popular
static type analyzers in the JavaScript community. All of OCaml’s features and
benefits cause it to be an incredible language and tool to use in modern day
applications, and is slowly taking the world by storm as developers realize how
powerful it is.&lt;/p&gt;
</description>
        <pubDate>Wed, 19 Oct 2016 17:53:40 -0700</pubDate>
        <link>/blog/whats-ocaml-and-why-you-should-care</link>
        <guid isPermaLink="true">http://github.com/pages/nickzuber/ssg-nickzuber.com/blog/whats-ocaml-and-why-you-should-care</guid>
        
        
      </item>
    
      <item>
        <title>Rolling Hash Data Structure and Applications</title>
        <description>&lt;p&gt;Let’s say you were given a massive text document and were allotted the task of finding a particular sentence somewhere inside it. This sentence may or may not actually &lt;em&gt;exist&lt;/em&gt; inside of this document, but it’s your job to try and find it anyways.&lt;/p&gt;

&lt;h2 id=&quot;basic-approaches&quot;&gt;Basic Approaches&lt;/h2&gt;

&lt;p&gt;The naive approach to this problem might be to take the sentence as a string, and compare it with every substring of the same size within the text document. It’s pretty clear that this solution is far from optimal; given that the text document is of size $n$ and the target string is of size $k$, this algorithm would have a asymptotic run time of $O(n \cdot k)$.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// Let D be equal to the text document
// Let K be equal to the substring we're looking for
for n := 0 to n - k
	if K == D.substring(n, n + k)
		return true
return false
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We can make this approach more practical by introducing hashing. So, let’s refer to our target string as the &lt;em&gt;key&lt;/em&gt; that we’re trying to find within our document. Given a hash function $h(S)$, we define the hashed value of the &lt;em&gt;key&lt;/em&gt; as $h_k$. Now, since the &lt;em&gt;key&lt;/em&gt; is of size $k$, we will define a subset of the document as a &lt;em&gt;window&lt;/em&gt; named $w$ that starts at the $n^{th}$ character and has a length of $k$ such that $w_n=\sum_{i=n}^{k}\ s_n$. From here, we need to compare $h_{w_n}$ and $h_k$ for all $n\ |\ 0 \leq n \leq n-k$ so that we end up with a set of &lt;em&gt;potential&lt;/em&gt; matches $P={p\in S\ |\ h_{w_n}=h_k}$. Assuming that we have a relatively good hash function, we can generally expect $| P |={1 \over | b |}$ such that $b$ is the image of $h(S;x)$.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// Let D be equal to the text document
// Let K be equal to the substring we're looking for
// Let h be a relatively good hash function
Hk := h(K)
for n := 0 to n - k
	if Hk == h(D.substring(n, n + k))
		if K == D.substring(n, n + k)
			return true
return false
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;As we traverse the text document looking for the key, each time we shift the window and compute a new hash value to compare to the key, we’re spend $O(k)$ time doing so due to the cost of recomputing the hash. If we do encounter a situation where $h_{w_n}=h_k$, we need to compare the two strings to ensure that we’ve indeed found a match and not a false positive – this process still takes $O(k)$ (remember that comparing strings is expensive since what’s actually going on under the hood is $k$ character comparisons). As long as the number of collisions is relatively low (which we’ve determined it is, given that we have a good hash function), then the entire process of finding the key within the text document is $O(n \cdot k)$.&lt;/p&gt;

&lt;p&gt;So at this point, it might be hard to see where the optimization comes in – even though we’ve traded the cost of string comparisons for number comparisons, we’ve consequently added the complexity of calculating and recalcuating hash values each time we want to compare the window hash to our key hash. This doesn’t seem to be any different from the original non-hashing approach (because it &lt;em&gt;isn’t&lt;/em&gt;), but this gives us an excellent segue into the rolling hash data structure.&lt;/p&gt;

&lt;h2 id=&quot;the-rolling-hash&quot;&gt;The Rolling Hash&lt;/h2&gt;

&lt;p&gt;Imagine we were given the same problem as before, and took the hashing approach. If we were able to rehash the window in $O(1)$ time, our algorithm’s overall runtime would drop from $O(n \cdot k)$ down to $O(n)$; this is a &lt;em&gt;significant&lt;/em&gt; improvement. We can actually accomplish this constant rehashing goal by using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Rolling_hash&quot;&gt;rolling hash&lt;/a&gt; data structure.&lt;/p&gt;

&lt;p&gt;To put it simply, a rolling hash is a data structure which contains the hashed value of some sequence of elements, and is able to update that hashed value by either appending the hashed value of any single new element or by removing the hashed value of a single existing element within the hashed sequence. You can think of this process as &lt;em&gt;hot loading&lt;/em&gt; but for a hashed value. With that being said, let’s lay out the abstract structure of what a rolling hash should look like – and keep in mind that these properties will all make sense once we start to unveil how it works:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;operations:
	skip(o: element)
	append(n: element)
	slide(o: element, n: element)
	hash(s: [ element ])
	set(s: [ element ])

internal properties:
	state: number
	BASE: number
	C: number
	INVERSE_BASE: number
	MODULAR_OFFSET: number
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;state&lt;/code&gt; is the internal variable that holds the current value of the hashed sequence. We typically refer to the sequence being hashed as the &lt;em&gt;window&lt;/em&gt;. Our main goal here is to try and figure out how we can shift the window through the entire text document, recomputing the hash in constant time.&lt;/p&gt;

&lt;h2 id=&quot;hashing-collections-efficiently&quot;&gt;Hashing Collections Efficiently&lt;/h2&gt;

&lt;p&gt;First, to formally define a &lt;em&gt;shift&lt;/em&gt; within the window, we’re talking about taking some window &lt;code class=&quot;highlighter-rouge&quot;&gt;w&lt;/code&gt;, such that $w \subset S$ where $S$ is defined as the sequence that we are performing the rolling hash operations on. We then go on to further define $w$ as its own sequence, starting at the $n^{th}$ character of $S$ and has a length of $k$ such that $w_n=\sum_{i=n}^{k}\ S_n$. A &lt;em&gt;shift&lt;/em&gt;, or &lt;em&gt;slide&lt;/em&gt;, operation on $w$ can be defined as an function $G(w;d)=w_{n+d}$ where $d$ is a fixed parameter that holds the value $-1$ or $+1$ respectively, depending on the direction you’re trying to shift $w$.&lt;/p&gt;

&lt;p&gt;Naturally, a shift operation would imply that we need to do two things: remove an element from $w$ and add an element to $w$. Let’s visualize this process with an example; instead of using strings, which would typically be base 256, we’re going to use a collection of base 10 numbers just because that’s easier to work with mathematically:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// The collection that we want to apply our rolling hash to.
S := [1, 2, 3, 4, 5, 6]

// Our window which contains a sub sequence of S.
// It can be any size, but we chose 3 here for the example.
w := [1, 2, 3]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We want our hashing function to first convert the sequence it’s given into a number. So we do something like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// Our hash function
//  - S: sequence (ex. [1, 2, 3])
//  - b: base     (ex. 10)
HASH(S, b)
	r := 0
	for i := S.length - 1 down to 0
		r: = r + (S[S.length - 1 - i] * b^i)
	return r;

// Hash our current window
wh := HASH(w) // =&amp;gt; 123
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We’ve concluded that the hash of our window is $h(w_n)=123$; therefore, using the same hashing function, we compute the hash value of the next window in our sequence $w_{n+1}=[2, 3, 4]$ which comes out to be $h(w_{n+1})=234$. Remember that our goal is to find a way to covert $h(w_n)$ to $h(w_{n+1})$, so we basically need to find a way to convert &lt;code class=&quot;highlighter-rouge&quot;&gt;123&lt;/code&gt; into &lt;code class=&quot;highlighter-rouge&quot;&gt;234&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To put into even simpler terms, we need to get rid of the &lt;code class=&quot;highlighter-rouge&quot;&gt;1&lt;/code&gt; from &lt;code class=&quot;highlighter-rouge&quot;&gt;123&lt;/code&gt; and then add a &lt;code class=&quot;highlighter-rouge&quot;&gt;4&lt;/code&gt;. To remove the &lt;code class=&quot;highlighter-rouge&quot;&gt;1&lt;/code&gt;, we need to subtract $123 - (1 \cdot 10^2) = 23$, which translates to $h(w_n)-(x \cdot b^{k-1})$ such that $b$ represents the base of the elements of $w$, $x$ being the element we’re trying to remove, and $k$ is the size or amount of digits in base $b$ within $w_n$. Now, adding the &lt;code class=&quot;highlighter-rouge&quot;&gt;4&lt;/code&gt; is trivial – we just need to multiply $w_n$ by $b$ and then just &lt;em&gt;add&lt;/em&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;4&lt;/code&gt;. This would look like $(23 \cdot 10) + 4$, which translates to $(h(w_n) \cdot b) + y$, where $y$ represents the element we’re adding.&lt;/p&gt;

&lt;p&gt;This helps us with building our sliding operation on $w$; we know that if we want to get from $h(w_n)$ to $h(w_{n+1})$, we need to perform the following operation on $w$: $h(w_{n+1})=(h(w_n)-(x \cdot b^{k-1})) \cdot b + y$ which can be reduced down to $h(w_{n+1})=h(w_n) \cdot b - x \cdot b^k + y$.&lt;/p&gt;

&lt;h2 id=&quot;modular-arithmetic&quot;&gt;Modular Arithmetic&lt;/h2&gt;

&lt;p&gt;Cool, so we have an expression for calculating a new hash value in some constant time. Now you’re probably thinking to yourself, ‘can we do better’? Of course we can! There are quite a few things about that rehash function that are immediate red flags; &lt;em&gt;lots&lt;/em&gt; of multiplication, tons of dependent variables, and most importantly the result is &lt;em&gt;completely unbounded&lt;/em&gt;. This implies that our rehash function is not very performant and can theoretically grow to infinity.&lt;/p&gt;

&lt;p&gt;One way of keeping this hash value controlled under some upper bound is to use &lt;a href=&quot;https://en.wikipedia.org/wiki/Modular_arithmetic&quot;&gt;modular arithmetic&lt;/a&gt;. Let’s assume we’ve picked a reasonably sized number $p$; we can now write our rehash function as $h(w_{n+1})=[h(w_n) \cdot b - x \cdot b^k + y] \mod p$. We can distribute the modulo and transform this method further: $h(w_{n+1})=[[h(w_n) \mod p] \cdot b - [x \cdot b^k \mod p] + y] \mod p$. To make things a little clearer, we can simplify this a bit since we know that our hash value is some number $h(w) \mod p$, we can just rewrite $h(w_n) \mod p \to h(w_n)$.&lt;/p&gt;

&lt;p&gt;Okay, so our current rehash function is a little better now. Our main goal here is to leave us with a hashing function that we can compute quickly in constant time, so some more refactoring is needed. We want to get rid of any large numbers and expensive operations (such as division, multiplication, and modulo) in our hashing function, since doing arithmetic with large numbers is slow. Let’s take a look at our current rehash function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(w_{n+1})=[h(w_n) \cdot b - x \cdot b^k \mod p + y] \mod p&lt;/script&gt;

&lt;p&gt;We’re looking for any expensive expressions within our algorithm, and we identify $b^k \mod p$ as a candidate. You might have already noticed, but this expression is a constant – which means we can precompute this value. So let us define a constant named $C=b^k \mod p$. So now, let’s take a look at our rehash function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(w_{n+1})=[h(w_n) \cdot b - x \cdot C + y] \mod p&lt;/script&gt;

&lt;p&gt;Since we know that $h(w)$, $b$, $x$, $y$, $p$, and $C$ are &lt;em&gt;all&lt;/em&gt; reasonable small numbers, we can conclude that this hashing function will compute relatively quickly in constant time. Woo!&lt;/p&gt;

&lt;h2 id=&quot;settings-things-up&quot;&gt;Settings Things Up&lt;/h2&gt;

&lt;p&gt;There are a few things we need to precompute and prepare before we start hammering away at the rolling hash methods. One of the most important parts of this process is to define what we want our hashing upper bound $p$ to be. To give us the best results, we want a number that will give us the least amount of collisions, while still being a reasonable size. In general, independent of a particular use case, we want $p$ to be as close to a typical &lt;a href=&quot;https://en.wikipedia.org/wiki/Word_(computer_architecture)&quot;&gt;word&lt;/a&gt; size as possible (which is usually $2^{32}$), and for a good hashing number, we want to use a &lt;a href=&quot;http://stackoverflow.com/a/1147232/5055063&quot;&gt;prime number that is as far away from a factor of 2 as possible&lt;/a&gt;. For the sake of an example, let’s set $p=3,221,225,533$, which is a good prime number between $2^{31}$ and $2^{32}$. Of course, this number can vary depending on the use case; but it is &lt;em&gt;important&lt;/em&gt; to make sure we use a prime number that is as far away from a factor of $2$ as possible.&lt;/p&gt;

&lt;p&gt;There are a few other things that we need to precompute, like the &lt;a href=&quot;https://en.wikipedia.org/wiki/Modular_multiplicative_inverse&quot;&gt;modular inverse&lt;/a&gt; of our base, and a particular modular offset. We’ll go over these and how to implement them when we get to it, but for now, just keep this in mind as something we’ll want to precompute ahead of time.&lt;/p&gt;

&lt;h2 id=&quot;breaking-down-the-methods&quot;&gt;Breaking Down the Methods&lt;/h2&gt;

&lt;p&gt;Now that we know the secret formula for the rolling hash, let’s break it down into its components and methods. Let’s start with the constructor:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;_constructor (base, p) {
	// The prime modular base for our rolling hash
	this.PRIME_BASE = p;

	// The base of the number system
	this.BASE = base;

	// The internal hash of the window
	this.state = 0;

	// The expensive bit of code we want to precompute.
	// Since this represents base^size mod p, it makes
	// sense to initialize this value to 1
	this.C = 1;

	// The modular inverse of our base
	this.INVERSE_BASE = modular_inverse(base, p) % p;

	// An offset we use to ensure that our computations
	// will not go negative.
	this.MODULAR_OFFSET = p * base;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;append&quot;&gt;Append&lt;/h4&gt;

&lt;p&gt;Now that our rolling hash is set up, let’s start with implementing the &lt;code class=&quot;highlighter-rouge&quot;&gt;append&lt;/code&gt; method. Let’s keep in mind that we’ve already figured out how to append an element to our window: $(h(w_n) \cdot b) + y$, where $y$ represents the element to add, so implementing the &lt;code class=&quot;highlighter-rouge&quot;&gt;append&lt;/code&gt; method should be relatively trivial; we just need to make sure to incorporate the modular math that we’ve already figured out. We also can’t forget to adjust our constant &lt;code class=&quot;highlighter-rouge&quot;&gt;C&lt;/code&gt; that we’re trying to precompute. Remember that $C=b^k \mod p$, where $k$ represents the size of our window. Since appending an element is consequently increasing our window size, we need to account for this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;append (y) {
	// Append the element
	this.state = (this.state * this.BASE + y) % this.PRIME_BASE;

	// Update our precomputed constant
	this.C = (this.C * this.BASE) % this.PRIME_BASE;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;skip&quot;&gt;Skip&lt;/h4&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;skip&lt;/code&gt; method is a little more involved than its &lt;code class=&quot;highlighter-rouge&quot;&gt;append&lt;/code&gt; counterpart; there are a few &lt;em&gt;gotcha&lt;/em&gt;’s here to look out for. Similarly to &lt;code class=&quot;highlighter-rouge&quot;&gt;append&lt;/code&gt;, we’ve already done the hard part and figured out how we skip or remove the trailing element from the hash: $h(w_n)-(x \cdot b^{k-1})$ (let’s not forget to incorporate the modular math we’ve already figured out as well). We need to do two things in our implementation of &lt;code class=&quot;highlighter-rouge&quot;&gt;skip&lt;/code&gt;; update the hash value and update the constant &lt;code class=&quot;highlighter-rouge&quot;&gt;C&lt;/code&gt;. Instead of trying to update the hash value first, &lt;em&gt;then&lt;/em&gt; updating &lt;code class=&quot;highlighter-rouge&quot;&gt;C&lt;/code&gt;, we can take advantage of the fact that we need to compute $b^{k-1}$ at some point to update the hash. This means we can update &lt;code class=&quot;highlighter-rouge&quot;&gt;C&lt;/code&gt; first, which will effectively result in $C=b^{k-1} \mod p$ before the method is complete.&lt;/p&gt;

&lt;p&gt;Remember when I said we needed to precompute some weird things like the inverse of our base and this mysterious modular offset? Well, you can scoot back from the edge of your seat, because you’re about to find out why.&lt;/p&gt;

&lt;p&gt;For &lt;code class=&quot;highlighter-rouge&quot;&gt;append&lt;/code&gt;, when we needed to update &lt;code class=&quot;highlighter-rouge&quot;&gt;C&lt;/code&gt;, all it took was increasing $b^k \mod p$ by a factor of $b$. However, for &lt;code class=&quot;highlighter-rouge&quot;&gt;skip&lt;/code&gt;, we need to &lt;em&gt;reduce&lt;/em&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;C&lt;/code&gt; by a factor of $b$. This is tricky because we’re dealing with numbers that are being affected by a modulo bound – we can’t just divide &lt;code class=&quot;highlighter-rouge&quot;&gt;C&lt;/code&gt; by $2$ here. The trick to solving this is to multiply &lt;code class=&quot;highlighter-rouge&quot;&gt;C&lt;/code&gt; by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Modular_multiplicative_inverse&quot;&gt;modular inverse&lt;/a&gt; of what we’re trying to decrease it by, in our case $b$.&lt;/p&gt;

&lt;p&gt;Another tricky &lt;em&gt;gotcha&lt;/em&gt; is how we’re performing subtraction. What’s tricky about subtraction, you ask? Well, when performing a calculation similar to $h(w_n)-(x \cdot b^{k-1})$, we need to realize that there is a possibility for $x \cdot b^{k-1}$ to be &lt;em&gt;greater&lt;/em&gt; than $h(w_n)$ – this would result in a negative number. When performing modular arithmetic on negative numbers, &lt;a href=&quot;http://math.stackexchange.com/a/519856/296221&quot;&gt;we expect to get a positive result&lt;/a&gt;. However, this behavior isn’t consistent throughout programming languages; for example, in JavaScript, &lt;code class=&quot;highlighter-rouge&quot;&gt;-5 % 2&lt;/code&gt; will return &lt;code class=&quot;highlighter-rouge&quot;&gt;-1&lt;/code&gt;. To combat this, we can take advantage of how modular arithmetic works. Since we’re taking the modulo of $p$, we can add any multiple of $p$ to our equation without changing the result. We can deduce that $x &amp;lt; b$ and that $b^{k-1} &amp;lt; p$, therefore $x \cdot b^{k-1}$ will always be smaller than $b \cdot p$. Because of this, by adding a factor of $b \cdot p$ to our equation, we can ensure that it never goes negative without affecting the outcome.&lt;/p&gt;

&lt;p&gt;So with all of this in mind, let’s build the &lt;code class=&quot;highlighter-rouge&quot;&gt;skip&lt;/code&gt; method:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;skip (x) {
	// Update our precomputed constant first
	this.C = (this.C * this.INVERSE_BASE) % this.PRIME_BASE;

	// Then, remove the element
	this.state = (this.state - x * this.C + this.MODULAR_OFFSET) % this.PRIME_BASE;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;slide&quot;&gt;Slide&lt;/h4&gt;

&lt;p&gt;If you’ve noticed, each time we &lt;code class=&quot;highlighter-rouge&quot;&gt;append&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;skip&lt;/code&gt; in our rolling hash, we have to update &lt;code class=&quot;highlighter-rouge&quot;&gt;C&lt;/code&gt;, which kind of defeats the purpose of having it precomputed, right? So in practice, when we have our window size all set and are ready to start rolling, we don’t want to &lt;code class=&quot;highlighter-rouge&quot;&gt;append&lt;/code&gt; n’ &lt;code class=&quot;highlighter-rouge&quot;&gt;skip&lt;/code&gt;, but rather take advantage of our precomputed constants and just &lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt;. Implementing the &lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; method is easy since we already did all of the work for it; recall when we solved our rehash function: $h(w_{n+1})=[h(w_n) \cdot b - x \cdot C + y] \mod p$.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;slide (x, y) {
	this.state = (this.state * this.BASE - x * this.C + y + this.MODULAR_OFFSET) % this.PRIME_BASE;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;set&quot;&gt;Set&lt;/h4&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;set&lt;/code&gt; method is very simple – just &lt;code class=&quot;highlighter-rouge&quot;&gt;append&lt;/code&gt; each element in the input.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;set (S) {
	for (let i=0; i&amp;lt;S.length; ++i) {
		this.append(S[i]);
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;hash&quot;&gt;Hash&lt;/h4&gt;

&lt;p&gt;This just refers to our &lt;em&gt;good&lt;/em&gt; hashing function. As long as we remember to hash responsibly and make sure each step keeps us within the bounds of $p$, we should be all set. That could look something like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hash (S) {
	var hash = 0;
	for (let i=0; i&amp;lt;S.length; ++i){
		hash += (S[i] % this.PRIME_BASE * (Math.pow(this.BASE, (S.length-1-i)) % this.PRIME_BASE) % this.PRIME_BASE);
	}
	return hash % this.PRIME_BASE;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;As arguably &lt;em&gt;one of&lt;/em&gt; the most efficient ways to search for or iterate sequentially over a collection of items, the rolling hash does serve us well. Also, you can see, once we’ve written out an implementation of this data structure, it becomes clear that we’re able to rehash in a relatively &lt;em&gt;good&lt;/em&gt; constant time; &lt;code class=&quot;highlighter-rouge&quot;&gt;append&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;skip&lt;/code&gt;, and especially &lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; are all $O(1)$ operations.&lt;/p&gt;

&lt;p&gt;If you’re interested in analyzing an &lt;a href=&quot;https://github.com/nickzuber/needle/blob/master/src/RollingHash/rollingHash.js&quot;&gt;implementation of a rolling hash in JavaScript&lt;/a&gt; that I wrote a little while back, feel free to check it out. I’ve also written various other data structures in JavaScript, keeping them nice and organized in a library called &lt;a href=&quot;https://github.com/nickzuber/needle&quot;&gt;Needle&lt;/a&gt; if you’re interested in giving them a look. Enjoy!&lt;/p&gt;
</description>
        <pubDate>Wed, 24 Aug 2016 21:19:09 -0700</pubDate>
        <link>/blog/rolling-hash-data-structure-and-applications</link>
        <guid isPermaLink="true">http://github.com/pages/nickzuber/ssg-nickzuber.com/blog/rolling-hash-data-structure-and-applications</guid>
        
        
      </item>
    
  </channel>
</rss>
